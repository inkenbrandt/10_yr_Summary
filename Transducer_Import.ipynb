{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T16:29:33.622017Z",
     "start_time": "2018-05-07T16:29:32.579856Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import matplotlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.ticker as tick\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T16:29:49.763768Z",
     "start_time": "2018-05-07T16:29:45.003567Z"
    }
   },
   "outputs": [],
   "source": [
    "#import loggerloader as ll\n",
    "import wellapplication as wa\n",
    "import arcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T16:29:49.775194Z",
     "start_time": "2018-05-07T16:29:49.771183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System Windows 10\n",
      "Python Version 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "Pandas Version 0.22.0\n",
      "Numpy Version 1.13.3\n",
      "Matplotlib Version 2.0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Operating System \" + platform.system() + \" \" + platform.release())\n",
    "print(\"Python Version \" + str(sys.version))\n",
    "print(\"Pandas Version \" + str(pd.__version__))\n",
    "print(\"Numpy Version \" + str(np.__version__))\n",
    "print(\"Matplotlib Version \" + str(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T16:29:50.838917Z",
     "start_time": "2018-05-07T16:29:50.835410Z"
    }
   },
   "outputs": [],
   "source": [
    "drive = 'G:'\n",
    "raw_archive_folder = drive + '/My Drive/WORK/Snake Valley'\n",
    "folder = raw_archive_folder + '/WaterMonitoring/PiezometerData/2017_2/Raw/'\n",
    "enteredFolder = folder + '/entered/'\n",
    "checkFolder = folder + '/toCheck/'\n",
    "#wellinfofile = drive + raw_archive_folder + '/well table 2015-03-23.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:01:56.423565Z",
     "start_time": "2018-01-16T16:01:56.410537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Folder Exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(enteredFolder):\n",
    "    print('Creating Output Folder')\n",
    "    os.makedirs(enteredFolder)\n",
    "else:\n",
    "    print('Output Folder Exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:01:56.443624Z",
     "start_time": "2018-01-16T16:01:56.436593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Folder Exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(checkFolder):\n",
    "    print('Creating Check Folder')\n",
    "    os.makedirs(checkFolder)\n",
    "else:\n",
    "    print('Check Folder Exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs for connection file and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:02:12.693474Z",
     "start_time": "2018-01-16T16:01:56.460655Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_file_root = \"C:/Users/PAULINKENBRANDT/AppData/Roaming/ESRI/Desktop10.5/ArcCatalog/\"\n",
    "conn_file = \"UGS_SDE.sde\" #production\n",
    "arcpy.env.workspace = conn_file_root + conn_file\n",
    "gw_reading_table = \"UGGP.UGGPADMIN.UGS_GW_reading\"\n",
    "station_table =  \"UGGP.UGGPADMIN.UGS_NGWMN_Monitoring_Locations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake Valley Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read over raw files -> append baro to db -> import well xle -> pull bp data from db -> remove bp press -> import manual measurements -> fix drift -> remove stickup -> include elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Barometric Data into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_table = \"UGGP.UGGPADMIN.UGS_NGWMN_Monitoring_Locations\"\n",
    "\n",
    "# create empty dataframe to house well data\n",
    "field_names = ['LocationID', 'LocationName', 'LocationType', 'LocationDesc', 'AltLocationID', 'Altitude',\n",
    "               'AltitudeUnits', 'WellDepth', 'SiteID', 'Offset', 'LoggerType', 'BaroEfficiency',\n",
    "               'BaroEfficiencyStart', 'BaroLoggerType']\n",
    "df = pd.DataFrame(columns=field_names)\n",
    "# populate dataframe with data from SDE well table\n",
    "search_cursor = arcpy.da.SearchCursor(loc_table, field_names)\n",
    "for row in search_cursor:\n",
    "    # combine the field names and row items together, and append them\n",
    "    df = df.append(dict(zip(field_names, row)), ignore_index=True)\n",
    "df.dropna(subset=['AltLocationID'],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xles = self.xle_head_table(self.xledir + '/')\n",
    "arcpy.AddMessage('xles examined')\n",
    "csvs = self.csv_info_table(self.xledir + '/')\n",
    "arcpy.AddMessage('csvs examined')\n",
    "file_info_table = pd.concat([xles, csvs[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_info_table(self, folder):\n",
    "    csv = {}\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    field_names = ['filename', 'Start_time', 'Stop_time']\n",
    "    df = pd.DataFrame(columns=field_names)\n",
    "    for file in files:\n",
    "        fileparts = os.path.basename(file).split('.')\n",
    "        filetype = fileparts[1]\n",
    "        basename = fileparts[0]\n",
    "        if filetype == 'csv':\n",
    "            try:\n",
    "                cfile = {}\n",
    "                csv[basename] = self.new_csv_imp(os.path.join(folder, file))\n",
    "                cfile['Battery_level'] = int(round(csv[basename].loc[csv[basename]. \\\n",
    "                                                   index[-1], 'Volts'] / csv[basename]. \\\n",
    "                                                   loc[csv[basename].index[0], 'Volts'] * 100, 0))\n",
    "                cfile['Sample_rate'] = (csv[basename].index[1] - csv[basename].index[0]).seconds * 100\n",
    "                cfile['filename'] = basename\n",
    "                cfile['fileroot'] = basename\n",
    "                cfile['full_filepath'] = os.path.join(folder, file)\n",
    "                cfile['Start_time'] = csv[basename].first_valid_index()\n",
    "                cfile['Stop_time'] = csv[basename].last_valid_index()\n",
    "                cfile['Location'] = ' '.join(basename.split(' ')[:-1])\n",
    "                cfile['trans type'] = 'Global Water'\n",
    "                df = df.append(cfile, ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "    df.set_index('filename', inplace=True)\n",
    "    return df, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['pw20 20170307.xle','pw03z 20170309.xle','pw01c 20170308.csv']\n",
    "file_extension = []\n",
    "for file in files:\n",
    "    file_extension.append(os.path.splitext(file)[1])\n",
    "'.xle' in file_extension and '.csv' in file_extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['pw20 20170307.xle','pw03z 20170309.xle','pw01c 20170308.csv']\n",
    "    if 'xle' in file_extension:\n",
    "            xles = self.xle_head_table(dirpath)\n",
    "            arcpy.AddMessage('xles examined')\n",
    "            file_info_table = xles\n",
    "        if 'csv' in file_extension:\n",
    "            csvs = self.csv_info_table(dirpath)\n",
    "            arcpy.AddMessage('csvs examined')\n",
    "            file_info_table = csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_info_table = pd.read_csv(r'M:\\PROJECTS\\Snake Valley Water\\Transducer Data\\Raw_data_archive\\2017\\file_info_table.csv')\n",
    "maxtime = max(pd.to_datetime(file_info_table['Stop_time']))\n",
    "mintime = min(pd.to_datetime(file_info_table['Start_time']))\n",
    "#arcpy.AddMessage(\"Data span from {:} to {:}.\".format(mintime,maxtime))\n",
    "\n",
    "# upload barometric pressure data\n",
    "baro_out = {}\n",
    "#baros = well_table[well_table['LocationType'] == 'Barometer']\n",
    "baros = []\n",
    "if len(baros) < 1:\n",
    "    baro_out['9003'] = get_location_data('9003', mintime, maxtime + datetime.timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location_data(site_number, first_date=None, last_date=None, limit=None,\n",
    "                      gw_reading_table=\"UGGP.UGGPADMIN.UGS_GW_reading\"):\n",
    "\n",
    "    if not first_date:\n",
    "        first_date = datetime.datetime(1900, 1, 1)\n",
    "    elif type(first_date) == str:\n",
    "        try:\n",
    "            datetime.datetime.strptime(first_date, '%m/%d/%Y')\n",
    "        except:\n",
    "            first_date = datetime.datetime(1900, 1, 1)\n",
    "    # Get last reading at the specified location\n",
    "    if not last_date or last_date > datetime.datetime.now():\n",
    "        last_date = datetime.datetime.now()\n",
    "    query_txt = \"LOCATIONID = '{:}' and (READINGDATE >= '{:%m/%d/%Y}' and READINGDATE <= '{:%m/%d/%Y}')\"\n",
    "    query = query_txt.format(site_number, first_date, last_date + datetime.timedelta(days=1))\n",
    "    sql_sn = (limit, 'ORDER BY READINGDATE ASC')\n",
    "\n",
    "    fieldnames = ll.get_field_names(gw_reading_table)\n",
    "    print(fieldnames)\n",
    "    readings = ll.table_to_pandas_dataframe(gw_reading_table, fieldnames, query, sql_sn)\n",
    "    print(readings)\n",
    "    readings.set_index('READINGDATE', inplace=True)\n",
    "    if len(readings) == 0:\n",
    "        arcpy.AddMessage('No Records for location {:}'.format(site_number))\n",
    "    return readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baro_out['9003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll.get_field_names(gw_reading_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.listdir(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wellimp.well_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xles = ll.xle_head_table(folder)\n",
    "csvs = ll.csv_info_table(folder)\n",
    "file_info_table = pd.concat([xles, csvs[0]])\n",
    "file_info_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wellid = '43'\n",
    "station_table =  \"UGGP.UGGPADMIN.UGS_NGWMN_Monitoring_Locations\"\n",
    "#arcpy.env.workspace = self.sde_conn\n",
    "loc_table = \"UGGP.UGGPADMIN.UGS_NGWMN_Monitoring_Locations\"\n",
    "\n",
    "field_names = ['LocationID', 'LocationName', 'LocationType', 'LocationDesc', 'AltLocationID', 'Altitude', \n",
    "               'AltitudeUnits', 'WellDepth', 'SiteID', 'Offset', 'LoggerType', 'BaroEfficiency', \n",
    "               'BaroEfficiencyStart', 'BaroLoggerType']\n",
    "df = pd.DataFrame(columns=field_names)\n",
    "\n",
    "# use a search cursor to iterate rows\n",
    "search_cursor = arcpy.da.SearchCursor(loc_table, field_names)\n",
    "\n",
    "    # iterate the rows\n",
    "for row in search_cursor:\n",
    "        # combine the field names and row items together, and append them\n",
    "    df = df.append(dict(zip(field_names, row)), ignore_index=True)\n",
    "    \n",
    "iddict = dict(zip(df['LocationName'].values,df['AltLocationID'].values))\n",
    "\n",
    "well_table = df.set_index(['AltLocationID'])\n",
    "baroid = well_table.loc[wellid,'BaroLoggerType']\n",
    "stickup = well_table.loc[wellid,'Offset']\n",
    "well_elev = well_table.loc[wellid,'Altitude']\n",
    "be = well_table.loc[wellid,'BaroEfficiency']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "['LocationID', 'LocationName', 'LocationType', 'LocationDesc', 'AltLocationID', 'Altitude', 'AltitudeUnits',\n",
    " 'WellDepth', 'SiteID', 'Offset', 'LoggerType', 'BaroEfficiency', 'BaroEfficiencyStart', 'BaroLoggerType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc = \"UGGP.UGGPADMIN.UGS_NGWMN_Monitoring_Locations\"\n",
    "field = \"LocationName\"\n",
    "cursor = arcpy.SearchCursor(fc)\n",
    "ll.table_to_pandas_dataframe(fc, field_names=['LocationName''AltLocationID'])\n",
    "for row in cursor:\n",
    "    print(row.getValue(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ.get('USERNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.path.splitext(well_file)[1] == '.xle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well_file = folder+'pw03z 20170309.xle'\n",
    "baro_file = folder+'pw03baro 20170309.xle'\n",
    "#ll.imp_one_well(well_file,baro_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well_table = ll.match_files_to_wellid(folder)\n",
    "#query barometers to a single table\n",
    "\n",
    "well_table.to_pickle(folder+'well_table.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpunits = []\n",
    "baroid = well_table.loc[wellid,'BaroLoggerType']\n",
    "stickup = well_table.loc[wellid,'Offset']\n",
    "well_elev = well_table.loc[wellid,'Altitude']\n",
    "be = well_table.loc[wellid,'BaroEfficiency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Transducer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well_table = ll.match_files_to_wellid(folder)\n",
    "#query barometers to a single table\n",
    "\n",
    "well_table.to_pickle(folder+'well_table.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxtime = max(pd.to_datetime(well_table['Stop_time']))\n",
    "mintime = min(pd.to_datetime(well_table['Start_time']))\n",
    "print('Pulling Barometric Pressure data from {:} to {:}'.format(mintime, maxtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpunits = []\n",
    "for ind in well_table.index:\n",
    "    if 'baro' in str(ind) or 'baro' in str(well_table.loc[ind,'Location']):\n",
    "        bpunits.append(well_table.loc[ind,'WellID'])\n",
    "\n",
    "baro_out = {}\n",
    "for baroid in bpunits:\n",
    "    baro_out[baroid] = ll.get_location_data(gw_reading_table, baroid, mintime, \n",
    "                                   maxtime + datetime.timedelta(days=1))\n",
    "    baro_out[baroid].to_pickle(folder+str(baroid)+'.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well_table =pd.read_pickle(folder+'well_table.pickle')\n",
    "\n",
    "bpunits = []\n",
    "for ind in well_table.index:\n",
    "    if 'baro' in str(ind) or 'baro' in str(well_table.loc[ind,'Location']):\n",
    "        bpunits.append(well_table.loc[ind,'WellID'])\n",
    "\n",
    "baro_out = {}\n",
    "for baroid in bpunits:\n",
    "    baro_out[baroid] = pd.read_pickle(folder + str(baroid) + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manualwls = raw_archive_folder + '/All tape measurements.csv'\n",
    "manual = pd.read_csv(manualwls, index_col=\"DateTime\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "man_startdate = '1/1/2001' \n",
    "man_endate = '10/11/2002'\n",
    "man_start_level = 10 \n",
    "man_end_level =15.1\n",
    "\n",
    "arcpy.AddMessage('Well {:} successfully imported!'.format(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T15:36:58.340231Z",
     "start_time": "2018-01-11T15:36:58.334698Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexedlist = [3]\n",
    "indexedlist[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well = ll.new_trans_imp(r'M:\\PROJECTS\\Snake Valley Water\\Transducer Data\\Raw_data_archive\\2017subset\\ag13a 20170308.xle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manualfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "manl = pd.read_csv(u'M:\\PROJECTS\\Snake Valley Water\\Transducer Data\\Raw_data_archive\\All tape measurements.csv')\n",
    "manualfile = manl[manl['Location ID'] == int('71')]\n",
    "manualfile.index = pd.to_datetime(manualfile.index)\n",
    "manualfile.sort_index(inplace=True)\n",
    "for i in range(len(manualfile)):\n",
    "    print(wa.fcl(well, manualfile.index[i]).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Well Data To SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barocolumn='MEASUREDLEVEL'\n",
    "dft_ln = {}\n",
    "dft_st = {}\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "pdf_pages = PdfPages(folder + '/wells.pdf')\n",
    "\n",
    "welltest = well_table.index.values\n",
    "for ind in welltest:\n",
    "    # import well file\n",
    "    df, man, be, drift = ll.imp_well(well_table,ind,manual,baro_out)\n",
    "    \n",
    "    # plot data\n",
    "    y1 = df['WATERELEVATION'].values\n",
    "    y2 = df['barometer'].values\n",
    "    x1 = df.index.values\n",
    "    x2 = df.index.values\n",
    "\n",
    "    x4 = man.index\n",
    "    y4 = man['Meas_GW_Elev']\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.scatter(x4,y4,color='purple')\n",
    "    ax1.plot(x1,y1,color='blue',label='Water Level Elevation')\n",
    "    ax1.set_ylabel('Water Level Elevation',color='blue')\n",
    "    ax1.set_ylim(min(df['WATERELEVATION']),max(df['WATERELEVATION']))\n",
    "    y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "    ax1.yaxis.set_major_formatter(y_formatter)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Barometric Pressure (ft)', color='red') \n",
    "    ax2.plot(x2,y2,color='red',label='Barometric pressure (ft)')\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=3)\n",
    "    plt.xlim(df.first_valid_index()-datetime.timedelta(days=3),df.last_valid_index()+datetime.timedelta(days=3))\n",
    "    plt.title('Well: {:}  Drift: {:}  Baro. Eff.: {:}'.format(ind,drift,be))\n",
    "\n",
    "    pdf_pages.savefig(fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "pdf_pages.close()\n",
    "print(\"DONE!\")\n",
    "print(\"Files in \"+ folder+'\\\\wells.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf_pages.savefig(fig)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "pdf_pages.close()\n",
    "print(\"DONE!\")\n",
    "print(\"Files in \"+ folder+'\\\\wells.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baro_eff(df, bp, wl, lag=200):\n",
    "    import statsmodels.tsa.tsatools as tools\n",
    "    df.dropna(inplace=True)\n",
    "    dwl = df[wl].diff().values[1:-1]\n",
    "    dbp = df[bp].diff().values[1:-1]\n",
    "    # dwl = df[wl].values[1:-1]\n",
    "    # dbp = df[bp].values[1:-1]\n",
    "    df['j_dates'] = df.index.to_julian_date()\n",
    "    lag_time = df['j_dates'].diff().cumsum().values[1:-1]\n",
    "    df.drop('j_dates', axis=1, inplace=True)\n",
    "    # Calculate BP Response Function\n",
    "\n",
    "    ## create lag matrix for regression\n",
    "    bpmat = tools.lagmat(dbp, lag, original='in')\n",
    "    ## transpose matrix to determine required length\n",
    "    ## run least squared regression\n",
    "    sqrd = np.linalg.lstsq(bpmat, dwl)\n",
    "    wlls = sqrd[0]\n",
    "    cumls = np.cumsum(wlls)\n",
    "    negcumls = [-1 * cumls[i] for i in range(len(cumls))]\n",
    "    ymod = np.dot(bpmat, wlls)\n",
    "\n",
    "    ## resid gives the residual of the bp\n",
    "    resid = [(dwl[i] - ymod[i]) for i in range(len(dwl))]\n",
    "    lag_trim = lag_time[0:len(cumls)]\n",
    "    return negcumls, cumls, ymod, resid, lag_time, dwl, dbp, wlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baro_eff(df,'barometer','Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "wl = 'cor2'\n",
    "y1 = df[wl].values\n",
    "y2 = df['barometer'].values\n",
    "x1 = df.index.values\n",
    "x2 = df.index.values\n",
    "\n",
    "x4 = man.index\n",
    "y4 = man['Meas_GW_Elev']\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.scatter(x4,y4,color='purple')\n",
    "ax1.plot(x1,y1,color='blue',label='Water Level Elevation')\n",
    "ax1.set_ylabel('Water Level Elevation',color='blue')\n",
    "ax1.set_ylim(min(df[wl]),max(df[wl]))\n",
    "y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "ax1.yaxis.set_major_formatter(y_formatter)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Barometric Pressure (ft)', color='red') \n",
    "ax2.plot(x2,y2,color='red',label='Barometric pressure (ft)')\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1+h2, l1+l2, loc=3)\n",
    "plt.xlim(df.first_valid_index()-datetime.timedelta(days=3),df.last_valid_index()+datetime.timedelta(days=3))\n",
    "plt.title('Well: {:}  Drift: {:}  Baro. Eff.: {:}'.format(ind,drift,be))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cor2'] = df[['Level', 'barometer']].\\\n",
    "        apply(lambda x: x[0] + 0.25 * (x[1]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['barometer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['Level'].diff(), df['barometer'].diff())\n",
    "import statsmodels.api as sm\n",
    "df['dbp'] = df['barometer'].diff()\n",
    "df['dwl'] = df['corrwl'].diff()\n",
    "df1 = df.dropna(subset=['dbp','dwl'])\n",
    "x = df1['dbp']\n",
    "y = df1['dwl']\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X).fit()\n",
    "    # y_reg = [data.ix[i,'Sbp']*m+b for i in range(len(data['Sbp']))]\n",
    "b = model.params[0]\n",
    "m = model.params[1]\n",
    "r = model.rsquared\n",
    "print(m,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "be, intc, r = ll.clarks(df[500:600],'barometer','corrwl')\n",
    "print(be,intc,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barometric Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:04:10.031695Z",
     "start_time": "2018-01-16T16:04:10.022703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold2014 = \"G:/My Drive/WORK/Snake Valley/WaterMonitoring/PiezometerData/2014_2/Raw/\"\n",
    "baro201402file = fold2014 + '1044548_2014_06_16.xle'\n",
    "baro2014a = barofileimp(baro201402file, 9024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addbaro = \"G:/My Drive/WORK/Snake Valley/Twini_Springs_Baro_9024.csv\"\n",
    "baro2014 = pd.read_csv(addbaro,index_col='READINGDATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baro = pd.concat([baro, baro2014])\n",
    "baro.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\tNephi Municipal Airport Weather Station KU14<br>\n",
    "<br>\n",
    "9027\tPW10<br>\n",
    "9049\tBarometer<br>\n",
    "<br>\n",
    "BARO3\t1038964\tGarrison PW03\t9003<br>\n",
    "BARO2\t1044788\tTwin Spring (Baro2)\t9024<br>\n",
    "BARO1\t1044779\tLelland-Harris\t9025<br>\n",
    "pw19baro\t1034820\n",
    "pw10baro\t1028270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:05:15.432425Z",
     "start_time": "2018-01-16T14:05:15.327596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compilation(inputfile):\n",
    "    \"\"\"This function reads multiple xle transducer files in a directory and generates a compiled Pandas DataFrame.\n",
    "    Args:\n",
    "        inputfile (file):\n",
    "            complete file path to input files; use * for wildcard in file name\n",
    "    Returns:\n",
    "        outfile (object):\n",
    "            Pandas DataFrame of compiled data\n",
    "    Example::\n",
    "        >>> compilation('O:\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\all\\\\LEV\\\\*baro*')\n",
    "        picks any file containing 'baro'\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionary to hold DataFrames\n",
    "    f = {}\n",
    "\n",
    "    # generate list of relevant files\n",
    "    filelist = glob.glob(inputfile)\n",
    "\n",
    "    # iterate through list of relevant files\n",
    "    for infile in filelist:\n",
    "        print(infile)\n",
    "        # get the extension of the input file\n",
    "        filetype = os.path.splitext(infile)[1]\n",
    "        # run computations using lev files\n",
    "        if filetype == '.lev':\n",
    "            # open text file\n",
    "            try:\n",
    "                with open(infile) as fd:\n",
    "                    # find beginning of data\n",
    "                    indices = fd.readlines().index('[Data]\\n')\n",
    "\n",
    "                # convert data to pandas dataframe starting at the indexed data line\n",
    "                f[wa.getfilename(infile)] = pd.read_table(infile, parse_dates=True, sep='\\s+', index_col=0,\n",
    "                                                       skiprows=indices + 2,\n",
    "                                                       names=['DateTime', 'Level', 'Temperature'],\n",
    "                                                       skipfooter=1, engine='python')\n",
    "                # add extension-free file name to dataframe\n",
    "                f[wa.getfilename(infile)]['name'] = wa.getfilename(infile)\n",
    "                f[wa.getfilename(infile)]['Level'] = pd.to_numeric(f[wa.getfilename(infile)]['Level'])\n",
    "                f[wa.getfilename(infile)]['Temperature'] = pd.to_numeric(f[wa.getfilename(infile)]['Temperature'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif filetype == '.xle':  # run computations using xle files\n",
    "            try:\n",
    "                f[wa.getfilename(infile)] = wa.new_xle_imp(infile)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif filetype == '.csv':\n",
    "            try:\n",
    "                f[wa.getfilename(infile)] = pd.read_csv(infile, parse_dates=0, index_col=0)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "    # concatenate all of the DataFrames in dictionary f to one DataFrame: g\n",
    "    g = pd.concat(f)\n",
    "    try:\n",
    "        g = g.reset_index()\n",
    "        g = g.set_index(['DateTime'])\n",
    "    except ValueError:\n",
    "        g.drop(['DateTime'],axis=1,inplace=True)\n",
    "        g = g.reset_index()\n",
    "        g = g.set_index(['DateTime'])\n",
    "    # drop old indexes\n",
    "    g = g.drop(['level_0'], axis=1)\n",
    "    # remove duplicates based on index then sort by index\n",
    "    g['ind'] = g.index\n",
    "    g.drop_duplicates(subset='ind', inplace=True)\n",
    "    g.drop('ind', axis=1, inplace=True)\n",
    "    g = g.sort_index()\n",
    "    outfile = g\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and compile existing baro data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search All files for Barometer Raw files and copy to a master barometer folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T13:48:14.086705Z",
     "start_time": "2018-01-16T13:47:51.530155Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "baronames = ['1038964','1044788','1044779','pw03baro','pw19baro',\n",
    "             '1034820','pw10baro','1028270',\n",
    "            'pw03 baro','pw19 baro','pw10 baro']\n",
    "dir = \"G:/My Drive/WORK/Snake Valley/Transducer Data/\"\n",
    "for pack in os.walk(dir):\n",
    "    #print(pack[0])\n",
    "    for baroname in baronames:\n",
    "        for i in glob.glob(pack[0]+'/'+'*{:}*'.format(baroname)):\n",
    "            if i[-4:] in ['.lev','.xle']:\n",
    "                rightfile = str(os.path.getmtime(i))+\"_\"+os.path.basename(i)\n",
    "                print(str(os.path.getmtime(i))+\"_\"+os.path.basename(i))\n",
    "                baro = \"G:/My Drive/WORK/Snake Valley/Barometers/\"\n",
    "                try:\n",
    "                    copyfile(i, os.path.join(baro, rightfile))\n",
    "                except:\n",
    "                    pass\n",
    "    #file_extension.append(os.path.splitext(file)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile barometers in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:19:00.961420Z",
     "start_time": "2018-01-16T14:18:10.264935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bro = {}\n",
    "for baroname in baronames:\n",
    "    bro[baroname] = compilation(baro+'/*{:}*'.format(baroname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:21:49.419530Z",
     "start_time": "2018-01-16T14:21:49.399481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pw3fill = pd.read_csv('G:/My Drive/WORK/Snake Valley/Barometers/pw03baro_2012.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:29:40.836109Z",
     "start_time": "2018-01-16T14:29:39.111448Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bro['pw03'] = pd.concat([bro['pw03baro'],bro['pw03 baro'],bro['1038964'],pw3fill])\n",
    "bro['pw03'].sort_index(inplace=True)\n",
    "bro['pw03'].drop_duplicates(inplace=True)\n",
    "bro['pw03'] = bro['pw03'][bro['pw03'].index > pd.datetime(2009,6,9)] \n",
    "bro['pw03']['Level'].plot()\n",
    "bro['pw03'].tail()\n",
    "bro['pw03'].to_csv(baro+'pw03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:31:48.732879Z",
     "start_time": "2018-01-16T14:31:47.038680Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bro['pw10'] = pd.concat([bro['pw10baro'],bro['pw10 baro'],bro['1028270']])\n",
    "bro['pw10'].drop_duplicates(inplace=True)\n",
    "bro['pw10'] = bro['pw10'][bro['pw10'].index > pd.datetime(2009,6,9)] \n",
    "bro['pw10']['Level'].plot()\n",
    "bro['pw10'].tail()\n",
    "bro['pw10'].to_csv(baro+'pw10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:31:06.294083Z",
     "start_time": "2018-01-16T14:31:04.918745Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bro['pw19'] = pd.concat([bro['pw19baro'],bro['pw19 baro'],bro['1034820']])\n",
    "bro['pw19'].drop_duplicates(inplace=True)\n",
    "bro['pw19']['Level'].plot()\n",
    "bro['pw19'].tail()\n",
    "bro['pw19'].to_csv(baro+'pw19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:36:13.436013Z",
     "start_time": "2018-01-16T14:36:12.549359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bro['1044779'].drop_duplicates(inplace=True)\n",
    "bro['1044779']['Level'].plot()\n",
    "bro['1044779'].tail()\n",
    "bro['1044779'].to_csv(baro+'1044779.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:40:48.626598Z",
     "start_time": "2018-01-16T14:40:47.765767Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bro['1044788'].drop_duplicates(inplace=True)\n",
    "bro['1044788']['Level'].plot()\n",
    "bro['1044788'].tail()\n",
    "bro['1044788'].to_csv(baro+'1044788.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read compiled data and upload new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:42:43.383677Z",
     "start_time": "2018-01-16T16:42:43.374652Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def barofileimp(df,altid):\n",
    "    #df = ll.new_trans_imp(bfile)\n",
    "    if 'name' in df.columns.values:\n",
    "        df.drop(['name'],axis=1,inplace=True)\n",
    "    if 'none' in df.columns.values:\n",
    "        df.drop(['none'],axis=1,inplace=True)\n",
    "    df.rename(columns = {'Temperature':'TEMP', 'Level':'MEASUREDLEVEL'},inplace=True)\n",
    "    df.index.name = 'READINGDATE'\n",
    "    df['TAPE'] = 0\n",
    "    df['LOCATIONID'] = altid\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:03:50.022852Z",
     "start_time": "2018-01-16T16:03:48.998052Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_csvs = ['pw03','1044788','1044779','pw19','pw10']\n",
    "\n",
    "barometerids = {'pw03':9003,'1044788':9024,'1044779':9025,'pw10':9027,'pw19':9049,'sg25':9061}\n",
    "barodict = {\"PW10 Barometer\":9027, \"PW19 Barometer\":9049, \"SG25 Barometer\":9061, \n",
    "            \"Leland-Harris Barometer\":9025, \"Twin Springs Barometer\":9024, \"PW03 Barometer\":9003}\n",
    "bpdict = {'pw03':'9003','pw10':'9027','pw19':'9049','twin':'9024','leland':'9025'}       \n",
    "\n",
    "barodata = {}\n",
    "\n",
    "barodrive = 'G:/My Drive/WORK/Snake Valley/Barometers/'\n",
    "\n",
    "head = ['MEASUREDLEVEL', 'TEMP', 'LOCATIONID']\n",
    "ind_head = ['READINGDATE']\n",
    "for csv in saved_csvs:\n",
    "    barodata[csv] = pd.read_csv(barodrive + '{:}.csv'.format(csv),\n",
    "                                index_col=0,parse_dates=True)\n",
    "    barodata[csv].sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pw03baro_append = folder + \"\\\\pw03 baro 2016-08-03.xle\"\n",
    "pw10baro_append = folder + \"\\\\pw10 baro 2016-08-03.xle\"\n",
    "pw19baro_append = folder + \"\\\\pw19 baro 2016-08-04.xle\"\n",
    "\n",
    "df = ll.new_trans_imp(bfile)\n",
    "barofileimp(bfile,altid)\n",
    "wa.appendomatic(pw03baro_append,pw03baro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:46:02.928450Z",
     "start_time": "2018-01-16T16:45:59.404257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pw03\n",
      "1044788\n",
      "1044779\n",
      "pw19\n",
      "pw10\n"
     ]
    }
   ],
   "source": [
    "for csv in saved_csvs:\n",
    "    print(csv)\n",
    "    df = barofileimp(barodata[csv], barometerids[csv])\n",
    "    maxinfo = ll.find_extreme(barometerids[csv])\n",
    "    if maxinfo[0] < df.index.max():\n",
    "        print(maxinfo,df.index.max())\n",
    "        \n",
    "        df = df[df.index > maxinfo[0]]#.to_csv(barodrive + '{:}_to_import.csv'.format(barometerids[csv]))\n",
    "        df.reset_index(inplace=True)\n",
    "        ll.edit_table(df, gw_reading_table, df.columns)                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baro.to_csv(raw_archive_folder + '\\\\baro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'ReadingID',\n",
    " 'WellID',\n",
    " 'DateTime',\n",
    " 'MeasuredLevel',\n",
    " 'Temp',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Level Tranducer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export and Plot Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Manual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manualwls = raw_archive_folder + \"\\\\All tape measurements.csv\"\n",
    "manual = pd.read_csv(manualwls, index_col=\"DateTime\", engine=\"python\")\n",
    "manualrecent = manual[manual.index.to_datetime() > pd.datetime(2015,6,1)]\n",
    "manualrecent.dropna(inplace=True)\n",
    "manualrecent.reset_index(inplace=True)\n",
    "print manualrecent.dtypes\n",
    "manualrecent = pd.merge(manualrecent, wellinfo, how='left',left_on='WellID', right_index=True)\n",
    "manualrecent.loc[:,\"MeasuredLevel\"] = np.nan\n",
    "manualrecent.loc[:,\"Temp\"] = np.nan\n",
    "manualrecent.loc[:,\"BaroEfficiencyCorrected\"] = np.nan \n",
    "manualrecent.loc[:,\"DeltaLevel\"] = np.nan\n",
    "manualrecent.loc[:,\"DriftCorrection\"] = np.nan\n",
    "manualrecent.loc[:,\"MeasuredBy\"] = np.nan\n",
    "manualrecent.loc[:,\"Tape\"] = 1\n",
    "manualrecent.loc[:,\"DTWBelowGroundSurface\"] = np.nan\n",
    "manualrecent.loc[:,\"WaterElevation\"] = np.nan\n",
    "#manualrecent[\"DTWBelowGroundSurface\"] = np.nan\n",
    "manualrecent.loc[:,\"DTWBelowCasing\"] = manualrecent.loc[:,\"MeasuredDTW\"]\n",
    "manualrecent.loc[:,\"DTWBelowGroundSurface\"] = manualrecent.loc[:,\"MeasuredDTW\"] - manualrecent.loc[:,\"Offset\"]\n",
    "manualrecent.loc[:,\"WaterElevation\"] = manualrecent.loc[:,'GroundElevation'] - manualrecent.loc[:,\"DTWBelowGroundSurface\"]\n",
    "print manualrecent\n",
    "\n",
    "#outpath = pathlist[0] + '\\\\' + pathlist[1] + '\\\\' + pathlist[2] + '\\\\' + pathlist[3] + '\\\\' + pathlist[4] + '\\\\' + 'Manual' + '.csv'  \n",
    "manualrecent.to_csv(raw_archive_folder+ 'Manual' + '.csv', index=True, columns= [\"WellID\",\"DateTime\",\"MeasuredLevel\",\"Temp\",\"BaroEfficiencyCorrected\",\"DeltaLevel\",\n",
    "                                             \"MeasuredDTW\",\"DriftCorrection\",\"DTWBelowCasing\",\"DTWBelowGroundSurface\",\n",
    "                                             \"WaterElevation\",\"Tape\",\"MeasuredBy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manual['DateTime'] = manual.index.to_datetime()\n",
    "manual.to_csv(raw_archive_folder+ 'Manual' + '.csv', index=False, columns= [\"WellID\",\"DateTime\",\"MeasuredLevel\",\"Temp\",\"BaroEfficiencyCorrected\",\"DeltaLevel\",\n",
    "                                             \"MeasuredDTW\",\"DriftCorrection\",\"DTWBelowCasing\",\"DTWBelowGroundSurface\",\n",
    "                                             \"WaterElevation\",\"Tape\",\"MeasuredBy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Files to Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print wellinfo.loc[:,'full_file_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files To Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manualwls = raw_archive_folder + '/All tape measurements.csv'\n",
    "manual = pd.read_csv(manualwls, index_col=\"DateTime\", engine=\"python\")\n",
    "barofile = raw_archive_folder + '/baro.csv'\n",
    "baro = pd.read_csv(barofile,index_col=0, parse_dates=True)\n",
    "wellinfo = pd.read_csv(folder + '/wellinfo4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = engineGetter.getEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Select Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T19:47:39.268579Z",
     "start_time": "2018-01-17T19:47:39.163298Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'widgets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7f534b0c24f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwelllist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"pw07mx 2015-11-30.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"pw02a 11-30-2015.xle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"pw02a 2016-07-07.xle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"sg23b 2016-05-02.xle\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwelllist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwelllist\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwellinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'full_file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'widgets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "form = widgets.VBox()\n",
    "welllist = [\"pw07mx 2015-11-30.csv\",\"pw02a 11-30-2015.xle\",\"pw02a 2016-07-07.xle\",\"sg23b 2016-05-02.xle\"]\n",
    "welllist = welllist+ list(wellinfo.loc[:,'full_file_name'].values)\n",
    "#print welllist\n",
    "\n",
    "wells = widgets.SelectMultiple(description=\"Well\", options=welllist, padding=4)\n",
    "pdfName = widgets.Text(description=\"PDF:\",padding=4)\n",
    "driftTol = widgets.FloatSlider(value=0.05, min=0.00, max=10.0, step=0.05, description='Drift Tolerance:')\n",
    "form.children = [wells, pdfName, driftTol]\n",
    "display(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in wells.value:\n",
    "    print folder+'/'+i\n",
    "    inputfile = folder +'/'+i\n",
    "    g, drift, wellname = svdi.imp_new_well(inputfile, wellinfo, manual, baro)\n",
    "    glist = g.columns.tolist()\n",
    "    y1 = g['WaterElevation'].values\n",
    "    y2 = baro['pw03'].values\n",
    "    x1 = g.index.values\n",
    "    x2 = baro.index.values\n",
    "    wellname, wellid = svdi.getwellid(folder+'\\\\'+i,wellinfo)\n",
    "    ylast = wellinfo[wellinfo['WellID']==wellid]['GroundElevation'].values[0] + wellinfo[wellinfo['WellID']==wellid]['Offset'].values[0] - svdi.fcl(manual[manual['WellID']== wellid],max(g.index.to_datetime()))[1]\n",
    "    yfirst = wellinfo[wellinfo['WellID']==wellid]['GroundElevation'].values[0] + wellinfo[wellinfo['WellID']==wellid]['Offset'].values[0] - svdi.fcl(manual[manual['WellID']== wellid],min(g.index.to_datetime()))[1]\n",
    "    xlast = pd.to_datetime((svdi.fcl(manual[manual['WellID']== wellid],max(pd.to_datetime(g.index)))).name)\n",
    "    xfirst = pd.to_datetime((svdi.fcl(manual[manual['WellID']== wellid],min(pd.to_datetime(g.index)))).name)\n",
    "    x4 = [xfirst,xlast]\n",
    "    y4 = [yfirst,ylast]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.scatter(x4,y4,color='purple')\n",
    "    ax1.plot(x1,y1,color='blue',label='Water Level Elevation')\n",
    "    ax1.set_ylabel('Water Level Elevation',color='blue')\n",
    "    y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "    ax1.yaxis.set_major_formatter(y_formatter)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Barometric Pressure (ft)', color='red') \n",
    "    ax2.plot(x2,y2,color='red',label='Barometric pressure (ft)')\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=3)\n",
    "    plt.xlim(xfirst-timedelta(days=3),xlast+timedelta(days=3))\n",
    "    plt.title('Well: ' + wellname.title() + '  ' + 'Total Drift = ' + str(g['DriftCorrection'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.workspace = \"C:/Users/PAULINKENBRANDT/AppData/Roaming/ESRI/Desktop10.4/ArcCatalog/UEMP_Dev.sde\"\n",
    "read_table = \"UEMP_Dev.UEMPADMIN.GW_reading\"\n",
    "\n",
    "arcpy.env.overwriteOutput=True\n",
    "edit = arcpy.da.Editor(env.workspace)\n",
    "edit.startEditing(False, True)\n",
    "edit.startOperation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf_pages = PdfPages(folder + '/' + pdfName.value + '.pdf')\n",
    "for i in wells.value:\n",
    "    print folder+'/'+i\n",
    "    inputfile = folder +'/'+i\n",
    "    g, drift, wellname = svdi.imp_new_well(inputfile, wellinfo, manual, baro)\n",
    "    quer = \"SELECT * FROM groundwater.reading where WellID = \" + str(g['WellID'].values[0]) + \" and DateTime > \\'\" + str(g.index.values[-1])[0:10] + \" \"+ str(g.index.values[-1])[11:19] + \"\\'\"\n",
    "\n",
    "    if abs(float(drift)) < driftTol.value:\n",
    "        if len(pd.read_sql_query(sql=quer,con=engine))<1:\n",
    "            g.to_csv(enteredFolder+wellname+\".csv\", index=False)\n",
    "            tablename = 'reading'\n",
    "            g.to_sql(con=engine, name = tablename, if_exists='append', flavor='mysql', index=False)\n",
    "            print(\"Added to DB table \" + tablename)\n",
    "        else:\n",
    "            print(\"Already Entered\")\n",
    "            print(len(pd.read_sql_query(sql=quer,con=engine)))\n",
    "    else:\n",
    "        g.to_csv(checkFolder+wellname+\".csv\", index=False, columns= [\"WellID\",\"DateTime\",\"MeasuredLevel\",\"Temp\",\"BaroEfficiencyCorrected\",\"DeltaLevel\",\n",
    "                                             \"MeasuredDTW\",\"DriftCorrection\",\"DTWBelowCasing\",\"DTWBelowGroundSurface\",\n",
    "                                             \"WaterElevation\",\"Tape\",\"MeasuredBy\"])\n",
    "        print(\"Check File\")\n",
    "    glist = g.columns.tolist()\n",
    "    for j in range(len(glist)):\n",
    "        if 'pw' in glist[j]:\n",
    "            h = glist[j]\n",
    "    y1 = g['WaterElevation'].values\n",
    "    y2 = baro['pw03'].values\n",
    "    x1 = g.index.values\n",
    "    x2 = baro.index.values\n",
    "    wellname, wellid = svdi.getwellid(folder+'\\\\'+i,wellinfo)\n",
    "    ylast = wellinfo[wellinfo['WellID']==wellid]['GroundElevation'].values[0] + wellinfo[wellinfo['WellID']==wellid]['Offset'].values[0] - svdi.fcl(manual[manual['WellID']== wellid],max(pd.to_datetime(g.index)))[1]\n",
    "    yfirst = wellinfo[wellinfo['WellID']==wellid]['GroundElevation'].values[0] + wellinfo[wellinfo['WellID']==wellid]['Offset'].values[0] - svdi.fcl(manual[manual['WellID']== wellid],min(pd.to_datetime(g.index)))[1]\n",
    "    xlast = pd.to_datetime(svdi.fcl(manual[manual['WellID']== wellid],max(pd.to_datetime(g.index))).name)\n",
    "    xfirst = pd.to_datetime(svdi.fcl(manual[manual['WellID']== wellid],min(pd.to_datetime(g.index))).name)\n",
    "    x4 = [xfirst,xlast]\n",
    "    y4 = [yfirst,ylast]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.scatter(x4,y4,color='purple')\n",
    "    ax1.plot(x1,y1,color='blue',label='Water Level Elevation')\n",
    "    ax1.set_ylabel('Water Level Elevation',color='blue')\n",
    "    y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "    ax1.yaxis.set_major_formatter(y_formatter)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Barometric Pressure (ft)', color='red') \n",
    "    ax2.plot(x2,y2,color='red',label='Barometric pressure (ft)')\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=3)\n",
    "    plt.xlim(xfirst-timedelta(days=3),xlast+timedelta(days=3))\n",
    "    plt.title('Well: ' + wellname.title() + '  ' + 'Total Drift = ' + str(g['DriftCorrection'][-1]))\n",
    "    \n",
    "    pdf_pages.savefig(fig)\n",
    "    plt.close()\n",
    "pdf_pages.close()\n",
    "print(\"DONE!\")\n",
    "print(\"Files in \"+ folder+'\\\\wells.pdf')\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "form = widgets.VBox()\n",
    "driftTol = widgets.FloatSlider(value=0.05, min=0.00, max=10.0, step=0.05, description='Drift Tolerance:')\n",
    "form.children = [driftTol]\n",
    "display(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf_pages = PdfPages(folder + '/wells.pdf')\n",
    "for i in wellinfo.loc[:,'full_file_name']:\n",
    "    print folder+'/'+i\n",
    "    inputfile = folder +'/'+i\n",
    "    g, drift, wellname = svdi.imp_new_well(inputfile, wellinfo, manual, baro)\n",
    "    quer = \"SELECT * FROM groundwater.reading where WellID = \" + str(g['WellID'].values[0]) + \" and (DateTime >= \\'\" + str(g.index.values[-1])[0:10] + \"\\')\"\n",
    "\n",
    "    if abs(float(drift)) < driftTol.value:\n",
    "        if len(pd.read_sql_query(sql=quer,con=engine))<1:\n",
    "            g.to_csv(enteredFolder + wellname+\".csv\", index=False)\n",
    "            g.to_sql(con=engine, name='reading', if_exists='append', flavor='mysql', index=False)\n",
    "            print(\"Added to DB\")\n",
    "        else:\n",
    "            print(\"Already Entered\")\n",
    "    else:\n",
    "        g.to_csv(checkFolder + wellname+\".csv\", index=False, columns= [\"WellID\",\"DateTime\",\"MeasuredLevel\",\"Temp\",\"BaroEfficiencyCorrected\",\"DeltaLevel\",\n",
    "                                             \"MeasuredDTW\",\"DriftCorrection\",\"DTWBelowCasing\",\"DTWBelowGroundSurface\",\n",
    "                                             \"WaterElevation\",\"Tape\",\"MeasuredBy\"])\n",
    "        print(\"Check File\")\n",
    "    glist = g.columns.tolist()\n",
    "    for j in range(len(glist)):\n",
    "        if 'pw' in glist[j]:\n",
    "            h = glist[j]\n",
    "    y1 = g['WaterElevation'].values\n",
    "    y2 = baro['pw03'].values\n",
    "    x1 = g.index.values\n",
    "    x2 = baro.index.values\n",
    "    wellname, wellid = svdi.getwellid(folder+'\\\\'+i,wellinfo)\n",
    "    ylast = wellinfo[wellinfo['WellID']==wellid]['GroundElevation'].values[0] + wellinfo[wellinfo['WellID']==wellid]['Offset'].values[0] - svdi.fcl(manual[manual['WellID']== wellid],max(pd.to_datetime(g.index)))[1]\n",
    "    yfirst = wellinfo[wellinfo['WellID']==wellid]['GroundElevation'].values[0] + wellinfo[wellinfo['WellID']==wellid]['Offset'].values[0] - svdi.fcl(manual[manual['WellID']== wellid],min(pd.to_datetime(g.index)))[1]\n",
    "    xlast = pd.to_datetime(svdi.fcl(manual[manual['WellID']== wellid],max(pd.to_datetime(g.index))).name)\n",
    "    xfirst = pd.to_datetime(svdi.fcl(manual[manual['WellID']== wellid],min(pd.to_datetime(g.index))).name)\n",
    "    x4 = [xfirst,xlast]\n",
    "    y4 = [yfirst,ylast]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.scatter(x4,y4,color='purple')\n",
    "    ax1.plot(x1,y1,color='blue',label='Water Level Elevation')\n",
    "    ax1.set_ylabel('Water Level Elevation',color='blue')\n",
    "    y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "    ax1.yaxis.set_major_formatter(y_formatter)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Barometric Pressure (ft)', color='red') \n",
    "    ax2.plot(x2,y2,color='red',label='Barometric pressure (ft)')\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=3)\n",
    "    plt.xlim(min(pd.to_datetime(g.index))-timedelta(days=3),max(pd.to_datetime(g.index))+timedelta(days=3))\n",
    "    plt.title('Well: ' + wellname.title() + '  ' + 'Total Drift = ' + str(g['DriftCorrection'][-1]))\n",
    "    \n",
    "    pdf_pages.savefig(fig)\n",
    "    plt.close()\n",
    "pdf_pages.close()\n",
    "print(\"DONE!\")\n",
    "print(\"Files in \"+ folder+'\\\\wells.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T21:07:48.924448Z",
     "start_time": "2018-01-17T21:07:37.773424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9062'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_file_root = \"C:/Users/PAULINKENBRANDT/AppData/Roaming/ESRI/Desktop10.5/ArcCatalog/\"\n",
    "conn_file = \"UGS_SDE.sde\" #production\n",
    "arcpy.env.workspace = conn_file_root + conn_file\n",
    "gw_reading_table = \"UGGP.UGGPADMIN.UGS_GW_reading\"\n",
    "stations_table =  \"UGGP.UGGPADMIN.UGS_NGWMN_Monitoring_Locations\"\n",
    "well_table = ll.table_to_pandas_dataframe(stations_table)\n",
    "\n",
    "well_table.set_index('AltLocationID',inplace=True)\n",
    "well_table = ll.table_to_pandas_dataframe(stations_table)\n",
    "well_table.set_index('AltLocationID',inplace=True)\n",
    "well_table.loc['7001', 'BaroLoggerType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T21:07:49.011653Z",
     "start_time": "2018-01-17T21:07:48.988593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manual_file = 'G:/My Drive/WORK/Snake Valley/manual_measurements.csv'\n",
    "manual = pd.read_csv(manual_file, index_col=\"DateTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T21:44:34.811332Z",
     "start_time": "2018-01-17T21:44:34.806819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9061', '9003', None, '9049', '9027', '9062'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_table['BaroLoggerType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:49:07.588472Z",
     "start_time": "2018-01-17T21:45:11.950434Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"LOCATIONID in('9061', '9003', '9049', '9027', '9062')\"\n",
    "baro_out = ll.table_to_pandas_dataframe(gw_reading_table,query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T21:44:15.625307Z",
     "start_time": "2018-01-17T21:15:42.529174Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c05c994d2af4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbaro\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"LOCATIONID = {:}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mbaro_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbaro\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable_to_pandas_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgw_reading_table\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\loggerloader\\utilities.py\u001b[0m in \u001b[0;36mtable_to_pandas_dataframe\u001b[1;34m(table, field_names, query, sql_sn)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_cursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;31m# combine the field names and row items together, and append them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;31m# return the pandas data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[0;32m   5192\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5193\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m-> 5194\u001b[1;33m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[0;32m   5195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5196\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                        copy=copy)\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5198\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5199\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[1;32m-> 5200\u001b[1;33m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[0;32m   5201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5202\u001b[0m             b = make_block(\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[1;34m(self, to_concat, placement)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m    322\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[1;32m--> 323\u001b[1;33m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[0;32m    324\u001b[0m         return self.make_block_same_class(\n\u001b[0;32m    325\u001b[0m             values, placement=placement or slice(0, len(values), 1))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baro_out = {}\n",
    "for baro in well_table['BaroLoggerType'].unique():\n",
    "    if baro is not None:\n",
    "        query = \"LOCATIONID = {:}\".format(baro)\n",
    "        baro_out[baro] = ll.table_to_pandas_dataframe(gw_reading_table,query=query)\n",
    "        print(baro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well_table['BaroLoggerType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j=0\n",
    "welly = {}\n",
    "\n",
    "welliddict = {'7002':'D3-41','7001':'F3-06'} \n",
    "wellids = ['7002','7001']\n",
    "beff,wells = {},{}\n",
    "for wellid in wellids:\n",
    "    for file in glob.glob(\"G:/My Drive/WORK/Juab/WellData/*{:}}*\".format(welliddict[wellid])):\n",
    "        print(file)\n",
    "        well = ll.new_trans_imp(file)\n",
    "        corrwl = ll.well_baro_merge(well, baro_out['9062'], barocolumn='MEASUREDLEVEL', vented=False)\n",
    "\n",
    "        wls, be = correct_be(wellid, well_table, corrwl,be=0.72)\n",
    "        stdata = well_table.loc[wellid,:]\n",
    "        man_sub = manual[manual['Location ID'] == int(wellid)]\n",
    "        well_elev = float(stdata['Altitude'])    \n",
    "        stickup = float(stdata['Offset'])\n",
    "        man_sub.loc[:, 'MeasuredDTW'] = man_sub['Water Level (ft)'] * -1\n",
    "        man_sub.loc[:, 'Meas_GW_Elev'] = man_sub['MeasuredDTW'].apply(lambda x: float(well_elev) + (x + float(stickup)),\n",
    "                                                                          1)\n",
    "        print('Stickup: {:}, Well Elev: {:}'.format(stickup, well_elev))\n",
    "\n",
    "        # fix transducer drift\n",
    "\n",
    "        dft = ll.fix_drift(wls, man_sub, meas='BAROEFFICIENCYLEVEL', manmeas='MeasuredDTW')\n",
    "        drift = np.round(float(dft[1]['drift'].values[0]), 3)\n",
    "\n",
    "        df = dft[0]\n",
    "        df.sort_index(inplace=True)\n",
    "        first_index = df.first_valid_index()\n",
    "        rowlist, fieldnames = ll.prepare_fieldnames(df, wellid, stickup, well_elev)\n",
    "        #beff[j] = m\n",
    "        wells[j] = rowlist \n",
    "        j += 1\n",
    "    welly[wellid] = pd.concat(wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldata = pd.concat(wells)\n",
    "alldata.drop('level_0',inplace=True,axis=1)\n",
    "alldata.reset_index(inplace=True)\n",
    "alldata.set_index('READINGDATE',inplace=True)\n",
    "alldata = alldata[alldata.index > pd.datetime(2015,5,15)]\n",
    "alldata = alldata[alldata['WATERELEVATION']>5145]\n",
    "alldata['WATERELEVATION'] = alldata['WATERELEVATION'].apply(lambda x: round(x,3),1)\n",
    "alldata.drop(['level_0','level_1','Level','corrwl','name','julian','datechange'],axis=1,inplace=True)\n",
    "alldata.drop(['barometer','dbp','dwl'],axis=1,inplace=True)\n",
    "alldata.sort_index(inplace=True)\n",
    "alldata.drop_duplicates(inplace=True)\n",
    "\n",
    "manual_file = 'G:/My Drive/WORK/Snake Valley/manual_measurements.csv'\n",
    "manual = pd.read_csv(manual_file, index_col=\"DateTime\",parse_dates=True)\n",
    "manual = manual[manual['Location ID']==7001]\n",
    "manual['wlelev'] = well_elev - (manual['Water Level (ft)'] - stickup)\n",
    "plt.plot(alldata.index, alldata['WATERELEVATION'],zorder=0)\n",
    "plt.scatter(manual.index, manual['wlelev'], color='red', zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T23:10:15.380575Z",
     "start_time": "2018-01-18T23:10:15.376563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['SITEID','DATETIME','A','B','C','D','Install Ground Elevation','STICKUP','CAP','DTW Current (ft)','WL_ELEV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T23:46:05.180675Z",
     "start_time": "2018-01-18T23:10:59.700905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-18_BARO1PiezometersCorrected2010_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-19_BARO2PiezometersCorrected2010_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-19_Baro3PiezometersCorrected2010_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-21_BARO1PiezometersCorrected2010_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-21_BARO2PiezometersCorrected.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-20_BARO3PiezometersCorrected_2010_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-27_BARO3PiezometersCorrected1051.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-29_BARO1PiezometersCorrected_2010_3.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-21_BARO2PiezometersCorrected_2010_3.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-21_BARO3PiezometersCorrected_2010_3.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-29_BARO1PiezometersCorrectedForNoWater2010.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-29_BARO1PiezometersCorrected_2010_4.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-07-02_BARO2PiezometersCorrected_2010_4.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-29_BARO3PiezometersCorrectedForNoWater2010.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-29_BARO3PiezometersCorrected_2010_4.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-07-12_PiezometersCorrected2011_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-27_BARO1PiezometersCorrectedForNoWater2010.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-07-18_PiezometersCorrected2011_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-07-11_PiezometersCorrected2011_2_OLD.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-07-26_PiezometersCorrected_2012_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-04_Baro1PiezometersCorrected.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-05_BARO2PiezometersCorrected.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-04_BARO3PiezometersCorrected.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-18_PiezometersCorrected_3_2012.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-18_PiezometersCorrected_2013_1 - Survey Elevation.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2013-10-31_PiezometersCorrected_2013_1 - LiDAR Elevation.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-07-01_PiezometersCorrected_2013_1 - Survey Elevation.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-08-28_PiezometersCorrected_2013_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-07-01_PiezometersCorrected_2013_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2016-12-27_PiezometersCorrected_2014_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2014-06-30_PiezometersCorrectedNeedCSVExport.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-07-01_PiezometersCorrected_2014_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-18_PiezometersCorrected2014_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-12-02_PiezometersCorrected2014_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-01-13_PiezometersCorrected2014_2_LELAND.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-04-20_PiezometersCorrected2014_2_NOTLELAND.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2016-12-29_PiezometersCorrected_2015_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-06-30_PiezometersCorrected_2015_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-12-02_PiezometersCorrected_2015_1a.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-19_PiezometersCorrected2015_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2016-09-15_PiezometersCorrected2015_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-12_PiezometersCorrected2016_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2016-12-22_PiezometersCorrected2016_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2016-09-21_PiezometersCorrected2016_120160921.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-17_PiezometersCorrected2016_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-11_PiezometersCorrected_MillsMona01052017.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2017-01-11_PiezometersCorrected2016_2_01112017.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2012-06-19_BARO1PiezometersCorrected2010_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2016-02-18_PiezometersCorrected (2).xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-06-30_PiezometersCorrected.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-07-01_PiezometersCorrected2014_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-12-02_PiezometersCorrected2015_2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\2015-08-25_PiezometersCorrected_2015_1.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\compiled.csv\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\~$compiled.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\compiled.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\compiledfield.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\compiledfield2.xlsx\n",
      "G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles\\desktop.ini\n"
     ]
    }
   ],
   "source": [
    "copydir = \"G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles/\"\n",
    "\n",
    "j = 0\n",
    "\n",
    "writer = pd.ExcelWriter('G:/My Drive/WORK/Snake Valley/CorrectedWetlandFiles/compiledfield3.xlsx')\n",
    "\n",
    "\n",
    "for xlfile in glob.glob(copydir+'/*'):\n",
    "    print(xlfile)\n",
    "    try:\n",
    "        df = pd.read_excel(xlfile,'PreviousDownloadLastLine',names=cols)\n",
    "        df.to_excel(writer, \"A\"+str(j))\n",
    "        writer.save()\n",
    "        j += 1\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {
    "height": "340px",
    "width": "312px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "613px",
    "left": "0px",
    "right": "963.2px",
    "top": "107px",
    "width": "268px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
